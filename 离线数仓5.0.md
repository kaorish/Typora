# 业务数据采集-全量同步

![image-20240229173704196](D:\My tepora note\离线数仓5.0.assets\image-20240229173704196.png)

在hadoop102的主目录的bin目录下，有gen_import_config.py、gen_import_config.sh和mysql_to_hdfs_full.sh三个脚本，其中gen_import_config.py就是用来干第一步的，gen_import_config.sh则是用于第二步，mysql_to_hdfs_full.sh用于第三步。

gen_import_config.py用于创建要同步的表的json模板文件。其创建出来的json文件中已经有了对于的字段名和类型等等，但是没有路径，需要我们在调用命令的时候动态传参，比如python bin/datax.py job/import/gmall.base_province.json -p"-Dtargetdir=/base_province"，其中-p"-Dtargetdir=XXX"的XXX就是我们需要上传到的HDFS中的路径，只不过需要提前创建好目标文件夹。在使用gen_import_config.py脚本生成模板文件时需要在命令中指明数据库名和表名，比如python bin/gen_import_config.py -d gmall -t base_province，-d后面跟的就是数据库名，我们这里一般都是用gmall数据库，-t后面跟的是表名，需要具体指定哪张表。此脚本生成出来的对于数据库中对于文件的json模板文件存放于/opt/module/datax/job/import/路径下。

gen_import_config.sh是用于一次性创建所有要同步的表的json文件，这里是15张，实际上可以根据需要生成无数张，因为原理很简单，就是要创建哪些表的json文件就执行多少个对应的gen_import_config.py命令（命令中需指明数据库和表名）。

![image-20240229174825047](D:\My tepora note\离线数仓5.0.assets\image-20240229174825047.png)

mysql_to_hdfs_full.sh就是用于一次性将所有的表都从mysql全量同步到hdfs。该脚本需要传两个参数，第一个是要同步的表，可以写表名也可以写all，all则一次性同步所有脚本中定义过的表，所以该脚本是可拓展的，我们可以根据需要在里面增加或修改表。第二个则是要归档的时间，可是为“年-月-日”，也可以不填，默认为当天日期减一，即昨天的日期。调用例子为mysql_to_hdfs_full.sh all 2020-06-14。当然，在生产环境下我们只需要传“all”这参数就可以了，因为生产环境下都是每天凌晨0点自动同步昨天的数据。该脚本同步到hdfs的地址是/origin_data/gmall/db/表名_full/日期，因为是业务数据，所以数据落到db上而非log，log是用于存放用户行为数据（也就是日志）的。



# 三个flume脚本的作用

我们一般把自己写的脚本放在家目录里的bin目录下，因为该目录配置了环境变量，可以全局调用。其中，与flume有关的脚本有三个，分别是 f1.sh 、f2.sh 和 f3.sh，他们的作用如下：

- f1.sh 是关于用户行为数据的，用于将用户行为数据（也就是日志）采集到kafka，在用户行为数据采集过程中属于上游flume。其中用户行为数据通过运行 log.sh 脚本生成，数据要发往的目标topic为topic_log。相关配置文件为 /opt/module/flume-1.9.0/job/file_to_kafka.conf。在用虚拟机配置的集群中，hadoop102和103作为上游flume，所以这种情况下f1.sh的作用对象是hadoop102和103。在云服务器作为单机模式时，上游flume是hadoop一个。
- f2.sh 也是关于用户行为数据的，用于将用户行为数据（也就是日志）从kafka采集到hdfs，在用户行为数据采集过程中属于下游flume。相关配置文件为 /opt/module/flume-1.9.0/job/kafka_to_hdfs_log.conf，消费的对象（topic）为topic_log。
- f3.sh 是关于业务数据的，准确来说是关于业务数据中需要进行增量同步的表（有的表采用全量同步）。Maxwell从MySQL获得需要进行增量同步的表增加的数据，被kafka消费（topic为topic_db），然后f3.sh启动flume消费kafka（topic为topic_db）的数据，传给hdfs，且增量同步的表的后缀为inc（即increase的缩写）。在增量同步业务数据中属于下游flume，此时的上游为Maxwell。相关的配置文件为 /opt/module/flume-1.9.0/job/kafka_to_hdfs_db.conf。

- 给flume写的三个拦截器的jar包上传到了 /opt/module/flume-1.9.0/lib 目录下，因为配置文件里的java类的全类名不同，所以这一个jar包就能同时解决三个flume脚本的问题（数据漂移和上传的文件日期不正确）。f1.sh的全类名是 `com.sunhao.gmall.flume.interceptor.ETLInterceptor$Builder` ,  f2.sh 的为`com.sunhao.gmall.flume.interceptor.TimestampInterceptor$Builder`，f3.sh 的为`com.sunhao.gmall.flume.interceptor.TimestampAndTableNameInterceptor$Builder` 。



# ports脚本

在使用云服务器搭建集群时我们总会遇到各种各样的问题，和我们平时使用vm虚拟机时有很大的区别，很大的原因就是云服务器不能关闭防火墙，因为要防止被挖矿，而我们平时学习时几乎一开始就关闭了防火墙，所以我们在生产环境里实际上要麻烦很多。

我们需要经常添加、关闭端口，但是一直输入命令很麻烦，于是我给自己造了个轮子，可以很方便地添加、删除、查看端口。

```shell
#!/bin/bash
if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi

case $1 in
"list")
	echo "已开放下列端口"
	firewall-cmd --list-ports

;;
"add")
	echo "==========添加端口=========="
	shift
	for i in $*
	do
        	echo "添加端口$i"
       		firewall-cmd --permanent --add-port=$i/tcp
	done

	echo "已重新加载"
	firewall-cmd --reload

;;
"reload")
	echo "已重新加载"
        firewall-cmd --reload

;;
"remove")
        echo "==========移除端口=========="
	shift
	for i in $*
        do
                echo "移除端口$i"
	firewall-cmd --zone=public --remove-port=$i/tcp --permanent
        done

        echo "已重新加载"
        firewall-cmd --reload
;;
*)
    echo "Input Args Error..."
;;
esac
```

该脚本很简单使用，分为添加、移除、查看端口和重新加载防火墙。而且在添加时会自动重新加载，从而立即生效。



# 关于ssh免密登录时hosts的配置

我们在配置ssh时一般一般会配置hosts，具体位于/etc/hosts，用户给ip取一个名字，就不用每次都敲一长串的ip地址了。但是我们需要注意，ssh一般要把自己算上。于是在配置hosts时需要注意如果是自己的话要用自己的内网ip，也就是使用ifconfig查看，而配置别的服务器时要使用公网ip，也就是使用xshell远程登录时的ip。自己想想也应该很好理解，因为自己连自己当然是通过内网咯，而连别人就是要在网路上发现对方，当然要使用外网。
