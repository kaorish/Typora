# 部署

## 前言

由于部署大模型需要极高的算力和超大的显存及内存，我们一般人的电脑几乎不可能跑起来。网上有句话是这样说的，“如果你是Linux环境并且有一张4090 24g显存的显卡，那么你可以尝试部署7B的模型。”可见，想要部署并运行大模型需要很强力的GPU，而且这仅仅是运行，如果想要进行fine-tuning（微调），那么所需占用的显存是单纯运行大模型时的2.5倍左右。以我为例，我运行大模型时显存占用15g，而fine-tuning期间占用38g。

于是我们选择部署在云端。这次我们选择”autoDL-算力云“这个GPU云服务器平台。这个平台的服务器没有公网IP，没有主机（在使用`systemctl status firewalld`查看防火墙状态时显示“host is down”），端口号只开放了6006，其余均不开放。这里我们租赁的GPU型号为A40 48g显存 80g内存的版本，2.68r/h。

我们选择的llama2模型为Chinese-Alpaca-2-7B-64K，alpaca是基于llama开源大模型对中文语料进行预训练后的版本，与官方模型相比对中文语料的接收和理解能力更强，此处我们用的是第二代alpaca，理论上更厉害。7B是指大模型里含有的参数数量，B是billion的缩写，即10亿。常见的大模型的参数为7B和13B，因为设备性能的限制，此处我们选择7B版本。64K表示一次对话中，问题＋答案一共的token数，即上下文长度。基准模型默认为4k，然后还有16k、32k、64k和128k的版本，16k及以上就属于长上下文模型，无法通过基准模型一样直接使用，需要配置辅助脚本或者PoRE 让模型发挥长上下文的功能。模型整体不大，仅仅12.9g，下载起来比较迅速



